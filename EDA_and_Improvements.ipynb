{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) & Improvements\n",
    "# Facial Emotion Recognition Project\n",
    "\n",
    "**Purpose**: Comprehensive data exploration, visualization, and model improvement recommendations\n",
    "\n",
    "**Author**: Enhancement Guide\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from collections import Counter\n",
    "import cv2\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—‚ï¸ Data Upload (Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running in Google Colab\n",
    "# from google.colab import files\n",
    "# import zipfile\n",
    "\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for filename in uploaded.keys():\n",
    "#     with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "#         zip_ref.extractall('.')\n",
    "#     print(f\"Extracted: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 1: Dataset Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_structure(data_dir='Train'):\n",
    "    \"\"\"\n",
    "    Analyze the basic structure and statistics of the dataset\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"DATASET STRUCTURE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    emotions = os.listdir(data_dir)\n",
    "    emotions = [e for e in emotions if os.path.isdir(os.path.join(data_dir, e))]\n",
    "    \n",
    "    stats_data = []\n",
    "    total_images = 0\n",
    "    \n",
    "    for emotion in emotions:\n",
    "        emotion_path = os.path.join(data_dir, emotion)\n",
    "        image_files = [f for f in os.listdir(emotion_path) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        count = len(image_files)\n",
    "        total_images += count\n",
    "        stats_data.append({'Emotion': emotion, 'Count': count})\n",
    "    \n",
    "    df_stats = pd.DataFrame(stats_data)\n",
    "    df_stats['Percentage'] = (df_stats['Count'] / total_images * 100).round(2)\n",
    "    df_stats = df_stats.sort_values('Count', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTotal Images: {total_images}\")\n",
    "    print(f\"Number of Classes: {len(emotions)}\\n\")\n",
    "    print(df_stats.to_string(index=False))\n",
    "    \n",
    "    # Calculate class imbalance ratio\n",
    "    max_count = df_stats['Count'].max()\n",
    "    min_count = df_stats['Count'].min()\n",
    "    imbalance_ratio = max_count / min_count\n",
    "    print(f\"\\nClass Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "    \n",
    "    if imbalance_ratio > 1.5:\n",
    "        print(\"âš ï¸  WARNING: Significant class imbalance detected!\")\n",
    "        print(\"   Recommendation: Use weighted loss or data balancing techniques\")\n",
    "    \n",
    "    return df_stats\n",
    "\n",
    "# Run analysis\n",
    "df_stats = analyze_dataset_structure('Train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Class Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_class_distribution(df_stats):\n",
    "    \"\"\"\n",
    "    Create comprehensive class distribution visualizations\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Bar plot\n",
    "    axes[0].bar(df_stats['Emotion'], df_stats['Count'], \n",
    "                color=plt.cm.Set3(range(len(df_stats))))\n",
    "    axes[0].set_xlabel('Emotion', fontsize=12)\n",
    "    axes[0].set_ylabel('Number of Images', fontsize=12)\n",
    "    axes[0].set_title('Class Distribution - Bar Chart', fontsize=14, fontweight='bold')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, (emotion, count) in enumerate(zip(df_stats['Emotion'], df_stats['Count'])):\n",
    "        axes[0].text(i, count + max(df_stats['Count'])*0.02, str(count), \n",
    "                    ha='center', fontweight='bold')\n",
    "    \n",
    "    # Pie chart\n",
    "    colors = plt.cm.Set3(range(len(df_stats)))\n",
    "    axes[1].pie(df_stats['Count'], labels=df_stats['Emotion'], autopct='%1.1f%%',\n",
    "                colors=colors, startangle=90)\n",
    "    axes[1].set_title('Class Distribution - Pie Chart', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Percentage comparison\n",
    "    axes[2].barh(df_stats['Emotion'], df_stats['Percentage'], \n",
    "                 color=plt.cm.Set3(range(len(df_stats))))\n",
    "    axes[2].set_xlabel('Percentage (%)', fontsize=12)\n",
    "    axes[2].set_ylabel('Emotion', fontsize=12)\n",
    "    axes[2].set_title('Class Distribution - Percentage', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, (emotion, pct) in enumerate(zip(df_stats['Emotion'], df_stats['Percentage'])):\n",
    "        axes[2].text(pct + 0.5, i, f'{pct}%', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize\n",
    "visualize_class_distribution(df_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 2: Image Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_properties(data_dir='Train', sample_size=100):\n",
    "    \"\"\"\n",
    "    Analyze image properties: dimensions, brightness, contrast, sharpness\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"IMAGE QUALITY ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    emotions = [e for e in os.listdir(data_dir) \n",
    "                if os.path.isdir(os.path.join(data_dir, e))]\n",
    "    \n",
    "    properties = {\n",
    "        'emotion': [],\n",
    "        'width': [],\n",
    "        'height': [],\n",
    "        'brightness': [],\n",
    "        'contrast': [],\n",
    "        'aspect_ratio': []\n",
    "    }\n",
    "    \n",
    "    for emotion in emotions:\n",
    "        emotion_path = os.path.join(data_dir, emotion)\n",
    "        image_files = os.listdir(emotion_path)[:sample_size]  # Sample for efficiency\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(emotion_path, img_file)\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('L')  # Grayscale\n",
    "                img_array = np.array(img)\n",
    "                \n",
    "                properties['emotion'].append(emotion)\n",
    "                properties['width'].append(img.width)\n",
    "                properties['height'].append(img.height)\n",
    "                properties['brightness'].append(img_array.mean())\n",
    "                properties['contrast'].append(img_array.std())\n",
    "                properties['aspect_ratio'].append(img.width / img.height)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    df_props = pd.DataFrame(properties)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nImage Dimension Statistics:\")\n",
    "    print(df_props[['width', 'height']].describe())\n",
    "    \n",
    "    print(\"\\nBrightness & Contrast Statistics:\")\n",
    "    print(df_props[['brightness', 'contrast']].describe())\n",
    "    \n",
    "    # Check for dimension inconsistencies\n",
    "    unique_dims = df_props.groupby(['width', 'height']).size()\n",
    "    if len(unique_dims) > 1:\n",
    "        print(\"\\nâš ï¸  WARNING: Multiple image dimensions detected!\")\n",
    "        print(\"   Recommendation: Ensure all images are resized consistently\")\n",
    "    \n",
    "    return df_props\n",
    "\n",
    "# Run analysis\n",
    "df_props = analyze_image_properties('Train', sample_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Image Property Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_properties(df_props):\n",
    "    \"\"\"\n",
    "    Visualize brightness, contrast, and aspect ratio distributions\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Brightness distribution by emotion\n",
    "    for emotion in df_props['emotion'].unique():\n",
    "        subset = df_props[df_props['emotion'] == emotion]['brightness']\n",
    "        axes[0, 0].hist(subset, alpha=0.6, label=emotion, bins=30)\n",
    "    axes[0, 0].set_xlabel('Brightness (Mean Pixel Intensity)', fontsize=11)\n",
    "    axes[0, 0].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[0, 0].set_title('Brightness Distribution by Emotion', fontsize=13, fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Contrast distribution by emotion\n",
    "    for emotion in df_props['emotion'].unique():\n",
    "        subset = df_props[df_props['emotion'] == emotion]['contrast']\n",
    "        axes[0, 1].hist(subset, alpha=0.6, label=emotion, bins=30)\n",
    "    axes[0, 1].set_xlabel('Contrast (Std Dev)', fontsize=11)\n",
    "    axes[0, 1].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[0, 1].set_title('Contrast Distribution by Emotion', fontsize=13, fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # Box plot: Brightness by emotion\n",
    "    df_props.boxplot(column='brightness', by='emotion', ax=axes[1, 0])\n",
    "    axes[1, 0].set_xlabel('Emotion', fontsize=11)\n",
    "    axes[1, 0].set_ylabel('Brightness', fontsize=11)\n",
    "    axes[1, 0].set_title('Brightness Comparison Across Emotions', fontsize=13, fontweight='bold')\n",
    "    plt.sca(axes[1, 0])\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Box plot: Contrast by emotion\n",
    "    df_props.boxplot(column='contrast', by='emotion', ax=axes[1, 1])\n",
    "    axes[1, 1].set_xlabel('Emotion', fontsize=11)\n",
    "    axes[1, 1].set_ylabel('Contrast', fontsize=11)\n",
    "    axes[1, 1].set_title('Contrast Comparison Across Emotions', fontsize=13, fontweight='bold')\n",
    "    plt.sca(axes[1, 1])\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.suptitle('')  # Remove auto-generated title\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize\n",
    "visualize_image_properties(df_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 3: Sample Images Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample_images(data_dir='Train', n_samples=5):\n",
    "    \"\"\"\n",
    "    Display grid of sample images from each class\n",
    "    \"\"\"\n",
    "    emotions = [e for e in os.listdir(data_dir) \n",
    "                if os.path.isdir(os.path.join(data_dir, e))]\n",
    "    emotions = sorted(emotions)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(emotions), n_samples, figsize=(12, 3*len(emotions)))\n",
    "    \n",
    "    for i, emotion in enumerate(emotions):\n",
    "        emotion_path = os.path.join(data_dir, emotion)\n",
    "        image_files = os.listdir(emotion_path)[:n_samples]\n",
    "        \n",
    "        for j, img_file in enumerate(image_files):\n",
    "            img_path = os.path.join(emotion_path, img_file)\n",
    "            img = Image.open(img_path).convert('L')\n",
    "            \n",
    "            ax = axes[i, j] if len(emotions) > 1 else axes[j]\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            if j == 0:\n",
    "                ax.set_title(f'{emotion.upper()}', fontsize=12, \n",
    "                           fontweight='bold', loc='left')\n",
    "    \n",
    "    plt.suptitle('Sample Images from Each Emotion Class', \n",
    "                fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples\n",
    "visualize_sample_images('Train', n_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 4: Pixel Intensity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pixel_distributions(data_dir='Train', sample_size=50):\n",
    "    \"\"\"\n",
    "    Analyze pixel intensity distributions across emotion classes\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PIXEL INTENSITY DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    emotions = [e for e in os.listdir(data_dir) \n",
    "                if os.path.isdir(os.path.join(data_dir, e))]\n",
    "    \n",
    "    pixel_stats = {}\n",
    "    \n",
    "    for emotion in emotions:\n",
    "        emotion_path = os.path.join(data_dir, emotion)\n",
    "        image_files = os.listdir(emotion_path)[:sample_size]\n",
    "        \n",
    "        all_pixels = []\n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(emotion_path, img_file)\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('L')\n",
    "                img_array = np.array(img).flatten()\n",
    "                all_pixels.extend(img_array)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        pixel_stats[emotion] = {\n",
    "            'mean': np.mean(all_pixels),\n",
    "            'median': np.median(all_pixels),\n",
    "            'std': np.std(all_pixels),\n",
    "            'min': np.min(all_pixels),\n",
    "            'max': np.max(all_pixels)\n",
    "        }\n",
    "    \n",
    "    df_pixel_stats = pd.DataFrame(pixel_stats).T\n",
    "    print(\"\\nPixel Intensity Statistics by Emotion:\")\n",
    "    print(df_pixel_stats.round(2))\n",
    "    \n",
    "    return pixel_stats\n",
    "\n",
    "# Run analysis\n",
    "pixel_stats = analyze_pixel_distributions('Train', sample_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 5: Data Augmentation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmentation_effects(data_dir='Train'):\n",
    "    \"\"\"\n",
    "    Show the effects of different augmentation techniques\n",
    "    \"\"\"\n",
    "    # Get a sample image\n",
    "    emotions = [e for e in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, e))]\n",
    "    sample_emotion = emotions[0]\n",
    "    sample_path = os.path.join(data_dir, sample_emotion)\n",
    "    sample_file = os.listdir(sample_path)[0]\n",
    "    image_path = os.path.join(sample_path, sample_file)\n",
    "    \n",
    "    # Define augmentations\n",
    "    augmentations = {\n",
    "        'Original': transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((48, 48))\n",
    "        ]),\n",
    "        'Horizontal Flip': transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((48, 48)),\n",
    "            transforms.RandomHorizontalFlip(p=1.0)\n",
    "        ]),\n",
    "        'Rotation (15Â°)': transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((48, 48)),\n",
    "            transforms.RandomRotation(15)\n",
    "        ]),\n",
    "        'Translation': transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((48, 48)),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.15, 0.15))\n",
    "        ]),\n",
    "        'Brightness': transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((48, 48)),\n",
    "            transforms.ColorJitter(brightness=0.3)\n",
    "        ]),\n",
    "        'Combined': transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((48, 48)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (name, transform) in enumerate(augmentations.items()):\n",
    "        transformed = transform(img)\n",
    "        axes[idx].imshow(transformed, cmap='gray')\n",
    "        axes[idx].set_title(name, fontsize=12, fontweight='bold')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Data Augmentation Effects', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize augmentations\n",
    "visualize_augmentation_effects('Train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 6: Model Performance Analysis\n",
    "## (Run this after training your model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_confusion_matrix(model, test_loader, class_names, device):\n",
    "    \"\"\"\n",
    "    Generate and visualize confusion matrix\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# Example usage (uncomment after training):\n",
    "# cm = analyze_confusion_matrix(model, test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 7: IMPROVEMENT IMPLEMENTATIONS\n",
    "## 7.1 Weighted Loss for Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights from dataset\n",
    "def compute_class_weights(dataset):\n",
    "    \"\"\"\n",
    "    Compute class weights for imbalanced dataset\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Get all labels\n",
    "    labels = [label for _, label in dataset]\n",
    "    class_counts = Counter(labels)\n",
    "    \n",
    "    # Calculate weights (inverse frequency)\n",
    "    total = sum(class_counts.values())\n",
    "    class_weights = {}\n",
    "    for cls, count in class_counts.items():\n",
    "        class_weights[cls] = total / (len(class_counts) * count)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    weights = torch.tensor([class_weights[i] for i in range(len(class_counts))], \n",
    "                          dtype=torch.float32)\n",
    "    \n",
    "    print(\"Class Weights:\")\n",
    "    for i, w in enumerate(weights):\n",
    "        print(f\"  Class {i}: {w:.4f}\")\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Example usage:\n",
    "# train_dataset = datasets.ImageFolder(root='Train', transform=train_transform)\n",
    "# class_weights = compute_class_weights(train_dataset)\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Enhanced Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced augmentation pipeline\n",
    "enhanced_train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    \n",
    "    # Existing augmentations\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    \n",
    "    # NEW: Additional augmentations\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    \n",
    "    # NEW: Random erasing (simulates occlusions)\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.15), ratio=(0.3, 3.3))\n",
    "])\n",
    "\n",
    "print(\"Enhanced augmentation pipeline created!\")\n",
    "print(\"New augmentations added:\")\n",
    "print(\"  âœ“ ColorJitter (brightness & contrast)\")\n",
    "print(\"  âœ“ RandomPerspective (viewpoint changes)\")\n",
    "print(\"  âœ“ GaussianBlur (noise robustness)\")\n",
    "print(\"  âœ“ RandomErasing (occlusion simulation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Transfer Learning with ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "def create_resnet_model(num_classes=4, pretrained=True):\n",
    "    \"\"\"\n",
    "    Create ResNet18 model adapted for grayscale emotion recognition\n",
    "    \"\"\"\n",
    "    # Load pretrained ResNet18\n",
    "    model = models.resnet18(pretrained=pretrained)\n",
    "    \n",
    "    # Modify first conv layer for grayscale input\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    \n",
    "    # Modify final layer for our number of classes\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "# resnet_model = create_resnet_model(num_classes=4).to(device)\n",
    "# print(f\"Total parameters: {sum(p.numel() for p in resnet_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Attention Mechanism (CBAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel Attention Module\"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        \n",
    "        # Average pooling\n",
    "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
    "        # Max pooling\n",
    "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
    "        \n",
    "        out = self.sigmoid(avg_out + max_out).view(b, c, 1, 1)\n",
    "        return x * out\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial Attention Module\"\"\"\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.sigmoid(self.conv(out))\n",
    "        return x * out\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    \"\"\"Convolutional Block Attention Module\"\"\"\n",
    "    def __init__(self, channels, reduction=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_attention = ChannelAttention(channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.channel_attention(x)\n",
    "        x = self.spatial_attention(x)\n",
    "        return x\n",
    "\n",
    "print(\"CBAM attention module created!\")\n",
    "print(\"Usage: Add after convolutional blocks to focus on important features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Advanced Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# AdamW optimizer (better weight decay)\n",
    "def create_adamw_optimizer(model, lr=0.001, weight_decay=0.01):\n",
    "    return optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# OneCycleLR scheduler\n",
    "def create_onecycle_scheduler(optimizer, max_lr, steps_per_epoch, epochs):\n",
    "    return optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr, \n",
    "        steps_per_epoch=steps_per_epoch, \n",
    "        epochs=epochs,\n",
    "        pct_start=0.3,  # Warm up for 30% of training\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "\n",
    "# Example usage:\n",
    "# optimizer = create_adamw_optimizer(model, lr=0.001, weight_decay=0.01)\n",
    "# scheduler = create_onecycle_scheduler(optimizer, max_lr=0.01, \n",
    "#                                       steps_per_epoch=len(train_loader), \n",
    "#                                       epochs=50)\n",
    "\n",
    "print(\"Advanced optimizer and scheduler created!\")\n",
    "print(\"  âœ“ AdamW: Better weight decay implementation\")\n",
    "print(\"  âœ“ OneCycleLR: Faster convergence with cyclic learning rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to prevent overfitting\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "    \n",
    "    def __call__(self, val_loss, model, path='checkpoint.pth'):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} â†’ {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# Example usage:\n",
    "# early_stopping = EarlyStopping(patience=7, verbose=True)\n",
    "# \n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     train_loss = train_epoch(...)\n",
    "#     val_loss = validate(...)\n",
    "#     \n",
    "#     early_stopping(val_loss, model)\n",
    "#     if early_stopping.early_stop:\n",
    "#         print(\"Early stopping triggered!\")\n",
    "#         break\n",
    "\n",
    "print(\"Early stopping implementation ready!\")\n",
    "print(\"Prevents overfitting by stopping when validation loss stops improving\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    Label smoothing to prevent overconfidence\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, preds, targets):\n",
    "        n_classes = preds.size(-1)\n",
    "        log_preds = F.log_softmax(preds, dim=-1)\n",
    "        \n",
    "        # Smooth labels\n",
    "        loss = -log_preds.sum(dim=-1)\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        \n",
    "        nll = F.nll_loss(log_preds, targets, reduction=self.reduction)\n",
    "        \n",
    "        return (1 - self.epsilon) * nll + self.epsilon * loss / n_classes\n",
    "\n",
    "# Example usage:\n",
    "# criterion = LabelSmoothingCrossEntropy(epsilon=0.1)\n",
    "\n",
    "print(\"Label smoothing loss function created!\")\n",
    "print(\"Prevents model from being overconfident in predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_gradient_clipping(model, loader, criterion, optimizer, device, max_norm=1.0):\n",
    "    \"\"\"\n",
    "    Training function with gradient clipping\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"Gradient clipping training function created!\")\n",
    "print(\"Prevents exploding gradients during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 8: Summary of Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Priority Recommendations (Quick Wins)\n",
    "\n",
    "### Immediate Actions (< 10 minutes each):\n",
    "1. **âœ“ Implement weighted loss** for class imbalance\n",
    "2. **âœ“ Add enhanced augmentations** (ColorJitter, RandomErasing)\n",
    "3. **âœ“ Use AdamW optimizer** instead of Adam\n",
    "4. **âœ“ Add gradient clipping** (prevents exploding gradients)\n",
    "5. **âœ“ Implement early stopping** (prevents overfitting)\n",
    "\n",
    "### Short-term Improvements (30-60 minutes):\n",
    "6. **âœ“ Try transfer learning** with ResNet18\n",
    "7. **âœ“ Implement label smoothing**\n",
    "8. **âœ“ Add OneCycleLR scheduler**\n",
    "9. **âœ“ Generate confusion matrix** for error analysis\n",
    "10. **âœ“ Add CBAM attention** to existing model\n",
    "\n",
    "### Long-term Enhancements:\n",
    "- Collect more diverse data\n",
    "- Implement ensemble methods\n",
    "- Deploy with FastAPI/Flask\n",
    "- Create mobile application\n",
    "- Add Grad-CAM visualization\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Expected Performance Improvements\n",
    "\n",
    "| Improvement | Expected Accuracy Gain | Difficulty |\n",
    "|------------|------------------------|------------|\n",
    "| Weighted Loss | +2-5% | Easy |\n",
    "| Enhanced Augmentation | +3-7% | Easy |\n",
    "| Transfer Learning (ResNet) | +5-10% | Medium |\n",
    "| Attention Mechanism | +2-4% | Medium |\n",
    "| Label Smoothing | +1-3% | Easy |\n",
    "| Better Optimizer (AdamW + OneCycle) | +2-5% | Easy |\n",
    "| **Combined Effect** | **+10-20%** | - |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”‘ Key Takeaways\n",
    "\n",
    "1. **Data Quality Matters**: Balance classes and augment appropriately\n",
    "2. **Modern Architectures Help**: ResNet >> Custom CNN from scratch\n",
    "3. **Regularization is Critical**: Use multiple techniques (dropout, label smoothing, etc.)\n",
    "4. **Monitor Everything**: Track metrics, visualize errors, analyze failures\n",
    "5. **Iterate Quickly**: Start simple, add complexity gradually\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Additional Resources\n",
    "\n",
    "- **FER Datasets**: FER2013, CK+, AffectNet, RAF-DB\n",
    "- **Papers**: \"Deep Facial Expression Recognition: A Survey\" (2020)\n",
    "- **Libraries**: timm (PyTorch Image Models), albumentations (augmentation)\n",
    "- **Tools**: Weights & Biases, TensorBoard, Netron (model visualization)\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your improvements! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
